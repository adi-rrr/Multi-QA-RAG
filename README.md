
 # ğŸ¤– RAG-Powered Multi-Agent Q&A Assistant

This project is a Retrieval-Augmented Generation (RAG) based question-answering assistant, enhanced with a simple agentic workflow. It retrieves relevant document snippets from a local knowledge base and uses a language model to generate natural-language answers. For specific types of queries (e.g. definitions or calculations), it routes to specialized tools.

---

## ğŸ“¦ Features

- Document ingestion and semantic chunking
- Vector search using FAISS
- LLM response generation (via OpenRouter API or local Hugging Face models)
- Agentic logic for routing queries to tools or RAG pipeline
- Streamlit web UI to interact with the assistant

---

## âš™ï¸ How It Works

1. **Ingestion**  
   - Upload or specify 3â€“5 text documents (e.g. FAQs, specs) in the `/data/` folder.  
   - `ingest.py` processes them into chunks and embeds them into vectors using a sentence-transformer model.

2. **Vector Storage & Retrieval**  
   - FAISS indexes the vector embeddings.  
   - `retrieve.py` searches for top-3 most relevant chunks based on semantic similarity to the userâ€™s question.

3. **LLM Integration**  
   - The answer is generated by an LLM using OpenRouter API or a local Hugging Face model.  
   - The context retrieved from FAISS is passed to the model as part of the prompt.

4. **Agentic Workflow (`router.py`)**  
   - If a query contains keywords like "define" or "calculate", itâ€™s routed to dictionary/calculator tools.  
   - Otherwise, it defaults to the RAG â†’ LLM pipeline.

5. **Streamlit UI**  
   - Run `app.py` to launch an interactive web UI.  
   - Displays the final answer, the tool used, and the retrieved document chunks.

---

## ğŸ§ª Project Structure

```

.
â”œâ”€â”€ app.py                # Streamlit UI
â”œâ”€â”€ ingest.py             # Document ingestion & chunking
â”œâ”€â”€ retrieve.py           # FAISS retrieval logic
â”œâ”€â”€ router.py             # Agent logic
â”œâ”€â”€ llm_engine.py        # Handles LLM API/local model
â”œâ”€â”€ data/                 # Text documents
â”œâ”€â”€ index.faiss           # FAISS index
â”œâ”€â”€ index.pkl             # Metadata store
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

and test files
```
## ğŸ”§ Setup Instructions

### 1. Clone the Repo

```bash
git clone https://github.com/YOUR_USERNAME/rag-multiagent-assistant.git
cd rag-multiagent-assistant
````

### 2. Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ”‘ OpenRouter API (Optional)

If you're using OpenRouter, create a `.env` file:

```
OPENROUTER_API_KEY=your-key
```

Or directly set the Bearer token in `llm_engine.py`.

---

## ğŸš€ Run the App

```bash
streamlit run app.py
```

Then open your browser at `http://localhost:8501`

---
