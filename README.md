
 # 🤖 RAG-Powered Multi-Agent Q&A Assistant

This project is a Retrieval-Augmented Generation (RAG) based question-answering assistant, enhanced with a simple agentic workflow. It retrieves relevant document snippets from a local knowledge base and uses a language model to generate natural-language answers. For specific types of queries (e.g. definitions or calculations), it routes to specialized tools.

---

## 📦 Features

- Document ingestion and semantic chunking
- Vector search using FAISS
- LLM response generation (via OpenRouter API or local Hugging Face models)
- Agentic logic for routing queries to tools or RAG pipeline
- Streamlit web UI to interact with the assistant

---

## ⚙️ How It Works

1. **Ingestion**  
   - Upload or specify 3–5 text documents (e.g. FAQs, specs) in the `/data/` folder.  
   - `ingest.py` processes them into chunks and embeds them into vectors using a sentence-transformer model.

2. **Vector Storage & Retrieval**  
   - FAISS indexes the vector embeddings.  
   - `retrieve.py` searches for top-3 most relevant chunks based on semantic similarity to the user’s question.

3. **LLM Integration**  
   - The answer is generated by an LLM using OpenRouter API or a local Hugging Face model.  
   - The context retrieved from FAISS is passed to the model as part of the prompt.

4. **Agentic Workflow (`router.py`)**  
   - If a query contains keywords like "define" or "calculate", it’s routed to dictionary/calculator tools.  
   - Otherwise, it defaults to the RAG → LLM pipeline.

5. **Streamlit UI**  
   - Run `app.py` to launch an interactive web UI.  
   - Displays the final answer, the tool used, and the retrieved document chunks.

---

## 🧪 Project Structure

```

.
├── app.py                # Streamlit UI
├── ingest.py             # Document ingestion & chunking
├── retrieve.py           # FAISS retrieval logic
├── router.py             # Agent logic
├── llm_engine.py        # Handles LLM API/local model
├── data/                 # Text documents
├── index.faiss           # FAISS index
├── index.pkl             # Metadata store
├── requirements.txt
└── README.md

and test files
```
## 🔧 Setup Instructions

### 1. Clone the Repo

```bash
git clone https://github.com/YOUR_USERNAME/rag-multiagent-assistant.git
cd rag-multiagent-assistant
````

### 2. Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## 🔑 OpenRouter API (Optional)

If you're using OpenRouter, create a `.env` file:

```
OPENROUTER_API_KEY=your-key
```

Or directly set the Bearer token in `llm_engine.py`.

---

## 🚀 Run the App

```bash
streamlit run app.py
```

Then open your browser at `http://localhost:8501`

---
